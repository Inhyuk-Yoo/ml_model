{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"report.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP8z8Puy5+YFO2QbCOrCl6C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"YbA1vOqiPrVv"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0kKWy051PxgN"},"source":["%cd /content/gdrive/MyDrive/2021_2학기/기계학습"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vzSapLLdPzu6"},"source":["!pwd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Yt2rZyJP0nJ"},"source":["!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IO7U_6wWP4rg"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","#import urllib.request\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **데이터 분석 및 정제**"],"metadata":{"id":"aQV2XY50P58g"}},{"cell_type":"code","metadata":{"id":"2xWk04J6P5Ef"},"source":["train_data = pd.read_csv('./data/train.csv', encoding = 'utf-8')\n","test_data = pd.read_csv('./data/test.csv', encoding = 'utf-8')\n","print('train data info :')\n","print(train_data.info())\n","print('test data info :')\n","print(test_data.info())"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check data length\n","print('train sample count : ', len(train_data))\n","print('test sample count : ', len(test_data))"],"metadata":{"id":"To2jqcth-Hf6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train data preview\n","train_data[:3]"],"metadata":{"id":"364-mM6G-H5p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test data preview\n","test_data[:3]"],"metadata":{"id":"3JWau2f3-vq1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# drop train data's duplicates\n","train_data.drop_duplicates(subset=['mail'], inplace=True)\n","print('train sample count (drop dup) : ', len(train_data))"],"metadata":{"id":"WZnW5My__YSo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prepare data (DataFrame -> Series)\n","label_data = train_data['label']\n","mail_data = train_data['mail']\n","mail_test = test_data['mail']\n","print('train label data info :')\n","print(type(label_data))\n","print(len(label_data))\n","print('train mail data info :')\n","print(type(mail_data))\n","print(len(mail_data))\n","print('test mail data info :')\n","print(type(mail_test))\n","print(len(mail_test))"],"metadata":{"id":"W9k8klC3AAGc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split data ( train & cv ) (test_size = 0.2)\n","mail_train, mail_cv, label_train, label_cv = train_test_split(mail_data, label_data, test_size=0.2, random_state=0, stratify=label_data)"],"metadata":{"id":"Lf3Fer7eBJ2M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **토큰화 및 정수 인코딩**"],"metadata":{"id":"uvUKHrCHBoXW"}},{"cell_type":"code","source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(mail_train)\n","mail_train_encoded = tokenizer.texts_to_sequences(mail_train)\n","print(type(mail_train_encoded))\n","print(mail_train_encoded)\n","\n","word_to_index = tokenizer.word_index\n","vocab_size = len(word_to_index) + 1\n","max_len = max(len(l) for l in mail_train_encoded)\n","mail_train_padded = pad_sequences(mail_train_encoded, maxlen = max_len)\n","# mail_train_padded : class 'numpy.ndarray'\n","print(type(mail_train_padded))\n","print(mail_train_padded)\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(mail_cv)\n","mail_cv_encoded = tokenizer.texts_to_sequences(mail_cv)\n","mail_cv_padded = pad_sequences(mail_cv_encoded, maxlen = max_len)\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(mail_test)\n","mail_test_encoded = tokenizer.texts_to_sequences(mail_test)\n","max_len_test = max(len(l) for l in mail_test_encoded)\n","mail_test_padded = pad_sequences(mail_test_encoded, maxlen = max_len_test)\n","print(type(mail_test_padded))\n","print(mail_test_padded)"],"metadata":{"id":"qhl-RcbQBdmN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(word_to_index)\n","print('vocabulary size : {}'.format((vocab_size)))\n","print('train data shape :', mail_train_padded.shape)\n","print('cv data shape :', mail_cv_padded.shape)\n","print('test data shape :', mail_test_padded.shape)"],"metadata":{"id":"Q2o7r2E-CdEv"},"execution_count":null,"outputs":[]}]}