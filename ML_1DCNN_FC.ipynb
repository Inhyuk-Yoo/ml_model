{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ML6_CNN_NLP.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNfYaDZuEIo/4DsSiYHJdyW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"ALx1siVttKp1"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ogP1eJXGtQDe"},"source":["%cd /content/gdrive/MyDrive/2021_2학기/기계학습"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MbmMALaitQ6H"},"source":["!pwd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HZOcKUJltRRu"},"source":["!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","train_data = pd.read_csv('./data/train.csv', encoding = 'utf-8')\n","test_data = pd.read_csv('./data/test.csv', encoding = 'utf-8')\n","train_data.drop_duplicates(subset=['mail'], inplace=True)\n","\n","label_data = train_data['label']\n","mail_data = train_data['mail']\n","mail_test = test_data['mail']\n","mail_train, mail_cv, label_train, label_cv = train_test_split(mail_data, label_data, test_size=0.2, random_state=0, stratify=label_data)\n","\n","######\n","def vectorize_sequences(sequences, dimension=41000):\n","    results = np.zeros((len(sequences), dimension))\n","    for i, sequence in enumerate(sequences):\n","        results[i, sequence] = 1.\n","    return results\n","#####\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(mail_train)\n","mail_train_encoded = tokenizer.texts_to_sequences(mail_train)\n","word_to_index = tokenizer.word_index\n","vocab_size = len(word_to_index) + 1\n","max_len = max(len(l) for l in mail_train_encoded)\n","# mail_train_padded = pad_sequences(mail_train_encoded, maxlen = max_len)\n","#####\n","mail_train_padded = vectorize_sequences(mail_train_encoded, 41000)\n","#####\n","\n","mail_cv_encoded = tokenizer.texts_to_sequences(mail_cv)\n","# mail_cv_padded = pad_sequences(mail_cv_encoded, maxlen = max_len)\n","#####\n","mail_cv_padded = vectorize_sequences(mail_cv_encoded, 41000)\n","#####\n","\n","mail_test_encoded = tokenizer.texts_to_sequences(mail_test)\n","max_len_test = max(len(l) for l in mail_test_encoded)\n","# mail_test_padded = pad_sequences(mail_test_encoded, maxlen = max_len_test)\n","#####\n","mail_test_padded = vectorize_sequences(mail_test_encoded, 41000)\n","#####"],"metadata":{"id":"f-bTsvQdyLWB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, Embedding, Dropout, MaxPooling1D, Activation\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras import backend as K\n","\n","def recall_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","\n","def precision_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","\n","def f1_m(y_true, y_pred):\n","    precision = precision_m(y_true, y_pred)\n","    recall = recall_m(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n","\n","##### model CNN\n","# embedding_dim = 32\n","# dropout_ratio = 0.5\n","# num_filters = 32\n","# kernel_size = 5\n","\n","# model = Sequential()\n","# model.add(Embedding(vocab_size, embedding_dim))\n","# model.add(Dropout(dropout_ratio))\n","# model.add(Conv1D(num_filters, kernel_size, strides=1, padding='valid', activation='relu'))\n","# model.add(GlobalMaxPooling1D())\n","# model.add(Dropout(dropout_ratio))\n","# model.add(Dense(1, activation='sigmoid'))\n","# model.summary()\n","#####\n","# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n","##### model neural network\n","model = Sequential()\n","model.add(Dense(8, activation='relu', input_shape=(41000,)))\n","model.add(Dense(8, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.summary()\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),loss='binary_crossentropy',metrics=['acc', f1_m, precision_m, recall_m])\n","# rmsprop, adam\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n","mc = ModelCheckpoint('best_model.h5', monitor='val_f1_m', mode='max', verbose=1, save_best_only=True)\n","\n","history = model.fit(mail_train_padded, label_train, epochs=100, batch_size=64, callbacks=[es, mc], validation_split=0.23)\n","# validation_split=0.2\n","\n","loss, accuracy, f1_score, precision, recall = model.evaluate(mail_cv_padded, label_cv, verbose=0)\n","print('loss: ')\n","print(loss)\n","print('accuracy: ')\n","print(accuracy)\n","print('f1_score: ')\n","print(f1_score)\n","print('precision: ')\n","print(precision)\n","print('recall: ')\n","print(recall)\n","\n","print(\"\\n cv acc : %.4f\" % (model.evaluate(mail_cv_padded, label_cv, batch_size=64)[1]))\n","#batch_size=100\n","\n","prediction = model.predict(mail_test_padded, batch_size=64)\n","for idx, p in enumerate(prediction):\n","    if p > 0.6 :\n","        prediction[idx] = bool(1)\n","    else :\n","        prediction[idx] = bool(0)\n","\n","submission = pd.read_csv('./result.csv', encoding = 'utf-8')\n","submission['label'] = prediction\n","submission[['label']]=submission[['label']].astype(int)\n","submission.to_csv('./submission_FC1.csv', index=False)\n","submission.info()\n","\n","epochs = range(1, len(history.history['acc']) + 1)\n","plt.plot(epochs, history.history['loss'])\n","plt.plot(epochs, history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()\n","\n","epochs = range(1, len(history.history['acc']) + 1)\n","plt.plot(epochs, history.history['acc'])\n","plt.plot(epochs, history.history['val_acc'])\n","plt.title('model acc')\n","plt.ylabel('acc')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()\n","\n","# from sklearn.metrics import classification_report\n","# print(classification_report(label_train, pred))"],"metadata":{"id":"FcHJdZ8gylgy"},"execution_count":null,"outputs":[]}]}