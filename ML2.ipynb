{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ML2.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPWOUmQIjsF+6AhVG9OP9sP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"DakjPa9izcx2"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"koME_5qFzqX_"},"source":["%cd /content/gdrive/MyDrive/2021_2학기/기계학습"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhqLwRxFzrzt"},"source":["!pwd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7GStsFUMzsZi"},"source":["!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lb-oy7gyzs1k"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import urllib.request\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zROX7VCs9tRn"},"source":["train_data = pd.read_csv('./train.csv', encoding = 'latin1')\n","print('train sample 수 :', len(train_data))\n","test_data = pd.read_csv('./test.csv', encoding = 'latin1')\n","print('test sample 수 :', len(test_data))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U2ISjFgXHdtH"},"source":["train_data[:5]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UAB8PIrjL2C9"},"source":["test_data[:5]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-MIqk2rfJM4U"},"source":["train_data.info()\n","print('train Null sample : ', train_data.isnull().values.any())\n","print('train is unique subject :', train_data['mail'].nunique())\n","# drop duplicates\n","train_data.drop_duplicates(subset=['mail'], inplace=True)\n","print('train sample number by dropping dup : ', len(train_data))\n","\n","test_data.info()\n","print('test Null sample : ', test_data.isnull().values.any())\n","print('test is unique subject :', test_data['mail'].nunique())\n","# drop duplicates\n","test_data.drop_duplicates(subset=['mail'], inplace=True)\n","print('test sample number by dropping dup : ', len(test_data))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZhCZnh9FKV6l"},"source":["train_data['label'].value_counts().plot(kind='bar')\n","print(train_data.groupby('label').size().reset_index(name='count'))\n","print(f'ham prop = {round(train_data[\"label\"].value_counts()[0]/len(train_data) * 100,3)}%')\n","print(f'spam prop = {round(train_data[\"label\"].value_counts()[1]/len(train_data) * 100,3)}%')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5uM_I4sOLLa9"},"source":["label_data = train_data['label']\n","mail_data = train_data['mail']\n","mail_train, mail_cv, label_train, label_cv = train_test_split(mail_data, label_data, test_size=0.2, random_state=0, stratify=label_data)\n","print('************* train vs cv **************')\n","print(f'train ham = {round(label_train.value_counts()[0]/len(label_train)*100,3)}%')\n","print(f'train spam = {round(label_train.value_counts()[1]/len(label_train)*100,3)}%')\n","\n","print(f'cv ham = {round(label_cv.value_counts()[0]/len(label_cv)*100,3)}%')\n","print(f'cv spam = {round(label_cv.value_counts()[1]/len(label_cv)*100,3)}%')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qZOoZyHlQxJ1"},"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(mail_train)\n","mail_train_encoded = tokenizer.texts_to_sequences(mail_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lqN8PHgGRIV7"},"source":["print(mail_train_encoded[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k0w7NGyZRM3h"},"source":["word_to_index = tokenizer.word_index\n","print(word_to_index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wK2bbzbYRlhz"},"source":["vocab_size = len(word_to_index) + 1\n","print('word size: {}'.format((vocab_size)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lg7q-FTzSMcw"},"source":["print('메일의 최대 길이 : %d' % max(len(l) for l in mail_train_encoded))\n","print('메일의 평균 길이 : %f' % (sum(map(len, mail_train_encoded))/len(mail_train_encoded)))\n","plt.hist([len(s) for s in mail_data], bins=50)\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0FCiZrozSeZi"},"source":["max_len = 2917\n","mail_train_padded = pad_sequences(mail_train_encoded, maxlen = max_len)\n","print(\"train data shape: \", mail_train_padded.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZscYnrc9Syac"},"source":["from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n","from tensorflow.keras.models import Sequential"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ijx3vu3lS_Xa"},"source":["model = Sequential()\n","model.add(Embedding(vocab_size, 32))\n","model.add(SimpleRNN(32))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","history = model.fit(mail_train_padded, label_train, epochs=6, batch_size=64, validation_split=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aOfpHp13WpQn"},"source":["mail_cv_encoded = tokenizer.texts_to_sequences(mail_cv)\n","mail_cv_padded = pad_sequences(mail_cv_encoded, maxlen = max_len)\n","print(\"\\n cv acc : %.4f\" % (model.evaluate(mail_cv_padded, label_cv)[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4xPpm0GxXYqQ"},"source":["epochs = range(1, len(history.history['acc']) + 1)\n","plt.plot(epochs, history.history['loss'])\n","plt.plot(epochs, history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]}]}